{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197783a4",
   "metadata": {},
   "source": [
    "# ü§ñ Fine-tuning OPT-125M for Question Answering\n",
    "This notebook demonstrates how to fine-tune the `facebook/opt-125m` model on a custom Question-Answering dataset using PyTorch and Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c579ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Install dependencies\n",
    "# !pip install transformers torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc183a",
   "metadata": {},
   "source": [
    "## 1. üì¶ Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e03bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5a011",
   "metadata": {},
   "source": [
    "## 2. üß† Load Pretrained OPT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87703de2",
   "metadata": {},
   "source": [
    "## 3. üìÇ Load and Preprocess QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e88908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA file format: question?[TAB]answer\n",
    "def load_qa_dataset(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"?\" in line:\n",
    "                parts = line.strip().split(\"?\", 1)\n",
    "                question = parts[0].strip() + \"?\"\n",
    "                answer = parts[1].strip()\n",
    "                qa_pairs.append(f\"Question: {question} Answer: {answer}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f8e77",
   "metadata": {},
   "source": [
    "## 4. ‚úÇÔ∏è Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(text, max_length=256):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd849bd7",
   "metadata": {},
   "source": [
    "## 5. üßæ Create PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_lines):\n",
    "        self.examples = []\n",
    "        for line in qa_lines:\n",
    "            tokenized = tokenize_function(line)\n",
    "            input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            self.examples.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v.to(device) for k, v in self.examples[idx].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c39ef",
   "metadata": {},
   "source": [
    "## 6. üß™ Load Data and Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content/sample_data/college_history.txt\"  # Update path as needed\n",
    "qa_lines = load_qa_dataset(file_path)\n",
    "dataset = QADataset(qa_lines)\n",
    "\n",
    "# Split into train and eval\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b4bd0",
   "metadata": {},
   "source": [
    "## 7. üèãÔ∏è‚Äç‚ôÄÔ∏è Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa15b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 6\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eafe0f",
   "metadata": {},
   "source": [
    "## 8. üíæ Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/content/opt_collegebot\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"‚úÖ Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55bc4ec",
   "metadata": {},
   "source": [
    "## 9. üí¨ Chatbot Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580172a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, max_length=100):\n",
    "    model.eval()\n",
    "    prompt = f\"Question: {question} Answer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766b706",
   "metadata": {},
   "source": [
    "## 10. üß™ Example Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43256fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"When was MANIT established?\"\n",
    "response = chat(user_question)\n",
    "print(\"üí¨ Bot Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
