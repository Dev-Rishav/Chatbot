{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9489c336",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.optimization import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load OPT model\n",
    "model_name = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load QA pairs from file (format: question[TAB]answer)\n",
    "def load_qa_dataset(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"?\" in line:\n",
    "                parts = line.strip().split(\"?\", 1)  # split only at the first ?\n",
    "                question = parts[0].strip() + \"?\"\n",
    "                answer = parts[1].strip()\n",
    "                qa_pairs.append(f\"Question: {question} Answer: {answer}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(text, max_length=256):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_lines):\n",
    "        self.examples = []\n",
    "        for line in qa_lines:\n",
    "            tokenized = tokenize_function(line)\n",
    "            input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            self.examples.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v.to(device) for k, v in self.examples[idx].items()}\n",
    "\n",
    "# Load and prepare dataset\n",
    "file_path = \"/content/sample_data/college_history.txt\"  # Each line should be: question<TAB>answer\n",
    "qa_lines = load_qa_dataset(file_path)\n",
    "dataset = QADataset(qa_lines)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training\n",
    "epochs = 6\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "save_path = \"/content/opt_collegebot\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "# Chatbot Function\n",
    "def chat(question, max_length=100):\n",
    "    model.eval()\n",
    "    prompt = f\"Question: {question} Answer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Interaction\n",
    "user_question = \"When was MANIT established?\"\n",
    "response = chat(user_question)\n",
    "print(\"ðŸ’¬ Bot Response:\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "# Save model\n",
    "save_path = \"/content/opt_collegebot\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "# Zip and download\n",
    "!zip -r /content/opt_collegebot.zip /content/opt_collegebot\n",
    "from google.colab import files\n",
    "files.download(\"/content/opt_collegebot.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
